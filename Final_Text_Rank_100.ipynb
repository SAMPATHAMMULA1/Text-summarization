{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Summarization for Extractive model- TextRank.**"
      ],
      "metadata": {
        "id": "4uFXi1iKMaKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Necessary Packages**"
      ],
      "metadata": {
        "id": "4WI90P-nOXeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm-x1pvay4T7",
        "outputId": "2183a13b-169c-4296-9fd7-a0c65e2bdff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import all Libraries**"
      ],
      "metadata": {
        "id": "1UntJ7W_MtdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3Cwx1qTicdB5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from rouge import Rouge\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from logging import logThreads\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set: Reddit_tifu"
      ],
      "metadata": {
        "id": "6xfLO7qdM0sY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrq01YO7osaT",
        "outputId": "8529d1b4-8c8a-46d9-861d-40973c12c794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ups: float32\n",
            "num_comments: float32\n",
            "upvote_ratio: float32\n",
            "score: float32\n",
            "documents: string\n",
            "tldr: string\n",
            "title: string\n"
          ]
        }
      ],
      "source": [
        "df = load_dataset(\"reddit_tifu\", \"long\")\n",
        "\n",
        "features = df[\"train\"].features\n",
        "\n",
        "for x, y in features.items():\n",
        "    print(f\"{x}: {y.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "32VBmILMpyGn"
      },
      "outputs": [],
      "source": [
        "df = df.map(lambda element: {'summary': element['tldr'], 'text': element['documents']})\n",
        "df = df.remove_columns([\"ups\", \"upvote_ratio\", \"num_comments\", \"score\", \"title\", \"tldr\", \"documents\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvhY0KcBClv2",
        "outputId": "f00af493-5279-4209-d27d-6fb34bde7f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary': ['confuse a 5th grade girl for a boy in front of half of her class. kids are mean. sorry sandra.**', 'i found my estranged dad, thought i loved him after getting to know him, got to know him better and changed my mind.', 'had my balls burned by sauron and was left deveeted.'], 'text': ['this actually happened a couple of years ago. i grew up in germany where i went to a german secondary school that went from 5th to 13th grade (we still had 13 grades then, they have since changed that). my school was named after anne frank and we had a club that i was very active in from 9th grade on, which was dedicated to teaching incoming 5th graders about anne franks life, discrimination, anti-semitism, hitler, the third reich and that whole spiel. basically a day where the students\\' classes are cancelled and instead we give them an interactive history and social studies class with lots of activities and games. \\n\\nthis was my last year at school and i already had a lot of experience doing these project days with the kids. i was running the thing with a friend, so it was just the two of us and 30-something 5th graders. we start off with a brief introduction and brainstorming: what do they know about anne frank and the third reich? you\\'d be surprised how much they know. anyway after the brainstorming we do a few activities, and then we take a short break. after the break we split the class into two groups to make it easier to handle. one group watches a short movie about anne frank while the other gets a tour through our poster presentation that our student group has been perfecting over the years. then the groups switch. \\n\\ni\\'m in the classroom to show my group the movie and i take attendance to make sure no one decided to run away during break. i\\'m going down the list when i come to the name sandra (name changed). a kid with a boyish haircut and a somewhat deeper voice, wearing clothes from the boy\\'s section at a big clothing chain in germany, pipes up. \\n\\nnow keep in mind, these are all 11 year olds, they are all pre-pubescent, their bodies are not yet showing any sex specific features one would be able to see while they are fully clothed (e.g. boobs, beards,...). this being a 5th grade in the rather conservative (for german standards) bavaria, i was confused. i looked down at the list again making sure i had read the name right. look back up at the kid. \\n\\nme: \"you\\'re sandra?\"\\n\\nkid: \"yep.\"\\n\\nme: \"oh, sorry. *thinking the kid must be from somewhere where sandra is both a girl\\'s and boy\\'s name* where are you from? i\\'ve only ever heard that as a girl\\'s name before.\"\\n\\nthe class starts laughing. sandra gets really quiet. \"i am a girl...\" she says. some of the other students start saying that their parents made the same mistake when they met sandra. i feel so sorry and stupid. i get the class to calm down and finish taking attendance. we watch the movie in silence. after the movie, when we walked down to where the poster presentation took place i apologised to sandra. i felt so incredibly terrible, i still do to this day. throughout the rest of the day i heard lots of whispers about sandra. i tried to stop them whenever they came up, but there was no stopping the 5th grade gossip i had set in motion.\\n\\nsandra, if you\\'re out there, i am so incredibly sorry for humiliating you in front of your class. i hope you are happy and healthy and continue to live your life the way you like. don\\'t let anyone tell you you have to dress or act a certain way just because of the body parts you were born with. i\\'m sorry if i made you feel like you were wrong for dressing and acting differently. i\\'m sorry i probably made that day hell for you. i\\'m sorry for my ignorance.', 'it was last october, but i\\'m feeling the fall-out from it the most today.  \\n\\nafter my mom died from cancer last year, i started looking for my estranged father.  i had only known him for a short time when i was 6 and barely remembered more than his name.  i never knew why he left and my mom would not talk about him without obvious emotional pain, so i didn\\'t push her for information about him.  i figured he didn\\'t care about us.\\n\\ni had googled him multiple times over the years, but he was always unlisted and i didn\\'t have an address with which to search.  after her death, i felt strongly that he should at least know that she died and that i have a beautiful little girl.  it felt wrong that he might be out there somewhere knowing nothing about us.\\ni googled him again and came up with a phone number and address that seemed realistic.  i called, figuring it was going to be another person by the same name since i\\'d already experienced that several times while looking for him.  it wasn\\'t.\\n\\ni had no idea how he would react to me, but he was ecstatic.  he told me that he had been a horrible alcoholic and that he didn\\'t pursue custody of me because he felt himself to be selfish and unstable.  he said he knew that my mom and my family would love me and take wonderful care of me (he was right about that!  my mom was the best woman i\\'ve ever known).\\n\\nso, we started talking and skyping.  we spent a lot of time learning about each other. knowing how he had hurt my mom and bailed on me made it difficult to see him as the person he has grown to be, but i tried to believe that people can change for the better.  \\n\\nhe was exactly everything i needed in a father my entire life:  supportive, sweet, funny, loving.  eventually, i told him that i loved him after months of talking with him.  he was over-the-moon.\\n\\nfast-forward to actually meeting him in person.  this is the part where, without knowing my dad or me, you might feel that i sound like an ass.  i say this because i don\\'t have a precise reason or event that i can point to as to why i don\\'t trust him.  why he actually makes me really nervous.\\n\\nmy dad is really touchy and hovers all of the time.  he doesn\\'t smell clean and his teeth are rotting and crumbled.  he doesn\\'t give personal space and, when i spent a couple of nights staying at his trailer, he kept coming in to my room to check on me while i was sleeping.  he said that he was just so excited to have me there that he couldn\\'t help it, but i really felt uncomfortable with it.  i couldn\\'t sleep and was so glad that i didn\\'t even consider bringing my daughter without spending time with him first.\\n\\nhe also asked me if he could kiss me on the mouth because his family does.  i said \"no\" and he accepted that, but the constant touchiness continued.  it wasn\\'t anything strictly inappropriate, but it made my skin crawl.  \\ni\\'ve spent very little actual time with him in person, maybe 2 weeks total over several trips.  i have, however, talked with him for countless hours on the phone and skype.  \\n\\nduring the most acute months of grieving for my mom, he was so devoted to me, trying to talk me through whatever he could.  i was so grateful for it that i kept ignoring details about his character that really concerned me.  he actually demanded a lot of my time, like hours and hours every day and would get upset if i cited my daughter and husband as reasons that i needed to chill with the phone.  he never paid my mom child support and obviously was never there for us my entire life.  he also evaded taxes and told me he had been to court 9 times and jail once.  \\n\\nagain, i know people can change, but at this point, i find myself wanting less contact with him.  the idea of him spending any time with my daughter alone is out of the question and my husband feels the same concerns.  i don\\'t want to hurt him, but i just can\\'t trust him.  i don\\'t even know if i\\'m asking for advice or confessing to being stupid and rushing into something that will hurt someone.\\n\\nthank you for reading and i sincerely want to wish all the wonderful father\\'s out there a very happy father\\'s day!\\n\\neta:', \"so i had the brilliant idea to use veet hair removal gel as the ol' danglies were in need of a trim and i had heard that veet leaves them smoother for longer. well i guess thats right when the skin has to grow back first. \\n\\npre-story: i tried this before and the results were great but i had decided to do it again because of how well it worked. \\n\\nso i take out the gel/cream stuff and just lob it on and cover the forest. the slight peroxide kinda smell ensued and all was going well, there was a slight burning but thats to be expected, you know, melting pubes off does that. so comes the all and mighty time of scraping that stuff off, top half all good, no pain or anything. but, the danglies were what was burning, i scraped them and withstood the pain because well lets face it, men are men and a little pain for balls as smooth as jam jars is fine right? wrong, it did great at taking the hair off it, but decided to take the first layer of skin too, my nuts now glow cherry red and walking is a painful expense that can't be justified.\\ni feel sauron has just tipped them in mount doom and dragged me balls first down the side of a rock wall.\"]}\n"
          ]
        }
      ],
      "source": [
        "print(df[\"train\"][:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Stop Words**"
      ],
      "metadata": {
        "id": "NJK-dQSmNMSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWqNM5FeybOT",
        "outputId": "336a9eae-65e1-4550-f61b-27ce8ad56c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LWUGq-Lq8Xgr"
      },
      "outputs": [],
      "source": [
        "# Below code is to initialize lemmatizer and also the stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Below code is to tokenize input text into sentences by NLTK's sent_tokenize.\n",
        "# And returns list of sentences.\n",
        "def gen_sentences(input):\n",
        "    return nltk.sent_tokenize(input)\n",
        "\n",
        "# Below code is to clean and lemmatize a list of sentences.\n",
        "# And returns list of sentences which are cleaned.\n",
        "def clean_sentences(input):\n",
        "  cs = []\n",
        "  for sentence in input:\n",
        "    sen = sentence.lower()\n",
        "    sen = re.sub(r'http\\S+|www\\S+|https\\S+', '', sen)\n",
        "    sen = re.sub(r'\\([^)]*\\)', '', sen)\n",
        "    sen = re.sub('\"','', sen)\n",
        "    sen = re.sub(\"[^a-zA-Z]\", \" \", sen)\n",
        "    cs.append(lemmatizer.lemmatize(sen))\n",
        "  return cs\n",
        "\n",
        "# Below code is to stop words from a sentence\n",
        "def delete_stopwords(input):\n",
        "    sen_new = \" \".join([i for i in input if i not in stop_words])\n",
        "    return sen_new"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading GloVe and unzipping the word embeddings dataset from Stanford.**"
      ],
      "metadata": {
        "id": "NsEFn0eYPJ9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxDyAo1CH_72",
        "outputId": "cc091183-d8ac-4fb3-ae51-105670944794"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-05 20:43:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-08-05 20:43:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-08-05 20:43:15--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-08-05 20:45:54 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below code is to store word embeddings\n",
        "word_embeddings = {}\n",
        "\n",
        "# Below code is to open the GloVe word embeddings file for reading\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        parts = line.split()\n",
        "        word, vector = parts[0], np.asarray(parts[1:], dtype='float32')\n",
        "        word_embeddings[word] = vector"
      ],
      "metadata": {
        "id": "46dwFwv0IYdA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lDvjx0XLles-"
      },
      "outputs": [],
      "source": [
        "# Below code is for text summary.\n",
        "def text_summary(text, summary_percentage):\n",
        "    sen = gen_sentences(text)\n",
        "    c_sen = clean_sentences(sen)\n",
        "    c_sen = [delete_stopwords(r.split()) for r in c_sen]\n",
        "\n",
        "    sentence_vectors = []\n",
        "    for sentence in c_sen:\n",
        "        words = sentence.split()\n",
        "        vector_sum = np.zeros((100,))\n",
        "\n",
        "        if len(words) > 0:\n",
        "            for word in words:\n",
        "                vector_sum += word_embeddings.get(word, np.zeros((100,)))\n",
        "\n",
        "            avg_vector = vector_sum / len(words)\n",
        "        else:\n",
        "            avg_vector = vector_sum\n",
        "\n",
        "        sentence_vectors.append(avg_vector)\n",
        "\n",
        "    len_sen = len(sen)\n",
        "    matrix = np.zeros([len_sen, len_sen])\n",
        "\n",
        "# Below code is to calculate cosine similarity scores between the sentence vectors\n",
        "    for i in range(len_sen):\n",
        "      for j in range(len_sen):\n",
        "        if i != j:\n",
        "          sc = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "          matrix[i][j] = sc\n",
        "\n",
        "    graph_matrix = nx.from_numpy_array(matrix)\n",
        "    sentence_scores = nx.pagerank(graph_matrix)\n",
        "\n",
        "    ranked_sentences = sorted(((sentence_scores[i],s) for i,s in enumerate(sen)), reverse=True)\n",
        "\n",
        "    num_sentences = max(1, int(summary_percentage * len(ranked_sentences)))\n",
        "\n",
        "    top_sentences = [sentence for _, sentence in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    return \" \".join(top_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kWjXnV_WhCyK"
      },
      "outputs": [],
      "source": [
        "# In below code we are initializing empty lists to store predicted and reference summaries\n",
        "predicted_summaries = []\n",
        "ref_summaries = []\n",
        "\n",
        "# Below code is to calculate predicted and reference summaries\n",
        "def calculation(df, num_rows, summary_percentage):\n",
        "    predicted_summaries = []\n",
        "    ref_summaries = []\n",
        "# Below code is to iterate over each row in the DataFrame\n",
        "    for i in range(num_rows):\n",
        "        ref_summaries.append(df['summary'][i])\n",
        "\n",
        "        p_summary = text_summary(df['text'][i], summary_percentage=summary_percentage)\n",
        "        predicted_summaries.append(p_summary)\n",
        "\n",
        "    return predicted_summaries, ref_summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation: Rouge Score**"
      ],
      "metadata": {
        "id": "imaOL7DSb6rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zSMzq0Zlt3_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a88e38a-8f1b-457a-8fe2-95c2668391e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1 score: 0.1697\n",
            "Average ROUGE-2 score: 0.0202\n",
            "Average ROUGE-L score: 0.1130\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_scores(pred, ori):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [scorer.score(pred, ori) for pred, ori in zip(pred, ori)]\n",
        "    return rouge_scores\n",
        "\n",
        "pred_s, ref_s = calculation(df[\"train\"], 100, 0.1)\n",
        "rouge_scores = calculate_rouge_scores(pred_s, ref_s)\n",
        "\n",
        "average_rouge1 = sum(scores['rouge1'].fmeasure for scores in rouge_scores) / len(rouge_scores)\n",
        "average_rouge2 = sum(scores['rouge2'].fmeasure for scores in rouge_scores) / len(rouge_scores)\n",
        "average_rougeL = sum(scores['rougeL'].fmeasure for scores in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "# Print the average ROUGE scores\n",
        "print(f\"Average ROUGE-1 score: {average_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2 score: {average_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L score: {average_rougeL:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}